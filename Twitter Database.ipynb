{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Twitter Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from election_helper import convert_datetime\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import os, io, json, glob, re, pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a Mongo database to store all of our tweets. This will allow us to easily query and access the tweets. We will create two collections, tweet_collection and user_collection. The tweet_collection will contain all of the tweets while the user_collection will contain all of the users. The tweet_collection will still contain the user information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define database globally\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client.election_tweets\n",
    "tweet_collection = db.tweets\n",
    "user_collection = db.users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimize Tweets\n",
    "\n",
    "There is a lot of unnessecary information stored within the tweets. The following function removes the unnessecary keys from the tweet and performs a few modifications on the JSON. A query and root_query field are added to keep track of which query the tweet came from. The root_query field has the place id removed. The tweet JSON contains an entities field which contains information about extra twitter objects in the tweet such as mentions and hashtags. This is flattened and any null fields are removed. The value of retweeted_status is either null if not a retweet or it contains the original retweet. If retweeted_status is not null, retweeted is set to true otherwise false. We then create an object which contains the original tweet if it exists. We then use this object to create a new database entry. The user is also extracted from the tweet and is used to create a seperate collection from the tweet collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def minimize_tweet(tweet, query):\n",
    "    # if a tweet doesn't have an id return none\n",
    "    if tweet.get('id', None) is None or tweet is None:\n",
    "        return None\n",
    "    \n",
    "    # helper func to remove unnessecary keys\n",
    "    \n",
    "    tweet_keys = ['contributors', 'geo', 'lang', 'id_str', 'metadata', 'in_reply_to_status_id_str',\n",
    "                'in_reply_to_user_id_str', 'quoted_status_id_str', 'notifications', 'truncated',\n",
    "                  'translator_type', 'contributors_enabled', 'default_profile', 'geo_enabled', \n",
    "                'has_extended_profile', 'source', 'coordinates', 'filter_level', 'possibly_sensitive', \n",
    "                'quoted_status_id_str','scopes', 'withheld_copyright']\n",
    "    \n",
    "    user_keys = [ 'contributors_enabled', 'default_profile', 'default_profile_image', 'follow_request_sent',\n",
    "                'geo_enabled', 'id_str', 'is_translator', 'lang', 'listed_count', 'notifications',\n",
    "                'profile_background_color', 'profile_banner_url', 'profile_background_tile',\n",
    "                'profile_background_image_url_https', 'profile_background_image_url', 'profile_image_url',\n",
    "                'profile_image_url_https', 'profile_image_url_https', 'profile_link_color', \n",
    "                'profile_sidebar_border_color', 'profile_sidebar_fill_color', 'profile_text_color',\n",
    "                'profile_use_background_image', 'show_all_inline_media', 'protected', 'url', 'utc_offset', \n",
    "                'time_zone', 'withheld_in_countries', 'withheld_scope', 'translator_type', 'entities',\n",
    "                'has_extended_profile', 'is_translation_enabled']\n",
    "    \n",
    "    place_keys = ['attributes', 'bounding_box', 'contained_within', 'place_type', 'url']\n",
    "    \n",
    "    def remove_keys(obj, keys):\n",
    "        if obj is None:\n",
    "            return None\n",
    "        \n",
    "        for key in keys:\n",
    "            obj.pop(key, None)\n",
    "        \n",
    "        # also sets id to _id for mongo\n",
    "        obj['_id'] = obj.pop('id')\n",
    "        \n",
    "        return obj\n",
    "    \n",
    "    # remove keys\n",
    "    tweet = remove_keys(tweet, tweet_keys)\n",
    "    \n",
    "    # remove place from query\n",
    "    pattern = '^(.*?)(?= place:|$)'\n",
    "    root_query = re.findall(pattern, query)[0]\n",
    "    \n",
    "    # set query key and from_query_root\n",
    "    tweet[u'query'] = query\n",
    "    tweet[u'root_query'] = root_query\n",
    "    \n",
    "    # handle entities\n",
    "    entities = tweet.get('entities', None)\n",
    "    if entities:\n",
    "        # remove entities from tweet\n",
    "        tweet.pop('entities', None)\n",
    "        # remove urls from entities\n",
    "        entities.pop('urls', None)\n",
    "        \n",
    "        # set entity objects directly to tweet\n",
    "        for entity, items in entities.iteritems():\n",
    "            # reduce keys from user mentions\n",
    "            if entity == 'user_mentions':\n",
    "                for i, obj in enumerate(items):\n",
    "                    items[i].pop('id_str')\n",
    "                    items[i].pop('name')\n",
    "                    items[i]['_id'] = items[i].pop('id')\n",
    "                    \n",
    "            if len(items) == 0:\n",
    "                tweet[entity] = None\n",
    "            else:\n",
    "                tweet[entity] = items\n",
    "                \n",
    "    # minimize the place field\n",
    "    tweet['place'] = remove_keys(tweet.get('place', None), place_keys)\n",
    "    \n",
    "    # minimize user field\n",
    "    user = remove_keys(tweet.get('user', None), user_keys)\n",
    "    tweet['user'] = user\n",
    "    \n",
    "    ### RETWEET has yet to be\n",
    "    # check if the tweet was retweeted\n",
    "    retweet = tweet.get('retweeted_status', None)\n",
    "    \n",
    "    # Set to None by default\n",
    "    retweet_container = None\n",
    "    \n",
    "    if retweet:\n",
    "        # set the retweeted field to true\n",
    "        tweet['retweeted'] = True\n",
    "        \n",
    "        # grab the retweet id\n",
    "        retweet_id = retweet.get('id', None)\n",
    "        \n",
    "        # Create retweet container to return\n",
    "        retweet_container = {\n",
    "            'retweet': retweet,\n",
    "            'id': retweet_id,\n",
    "            # save the time the retweet was made, can use to compare vs a tweet saved in the data base\n",
    "            'created_at': convert_datetime(tweet.get('created_at'))\n",
    "        }\n",
    "        \n",
    "        # set retweeted_status to the id of the orginal retweet\n",
    "        tweet['retweeted_status'] = retweet_id\n",
    "    \n",
    "    return (tweet, user, retweet_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an exmaple of a minimized tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'_id': 795670816545456129,\n",
      "   u'created_at': u'Mon Nov 07 16:54:40 +0000 2016',\n",
      "   u'favorite_count': 0,\n",
      "   u'favorited': False,\n",
      "   u'hashtags': None,\n",
      "   u'in_reply_to_screen_name': None,\n",
      "   u'in_reply_to_status_id': None,\n",
      "   u'in_reply_to_user_id': None,\n",
      "   u'is_quote_status': False,\n",
      "   u'place': {'_id': u'0064529d5fb32582',\n",
      "              u'country': u'Canada',\n",
      "              u'country_code': u'CA',\n",
      "              u'full_name': u'Georgian Bluffs, Ontario',\n",
      "              u'name': u'Georgian Bluffs'},\n",
      "   u'query': '#Trump OR #Hilary place:c23',\n",
      "   u'retweet_count': 1,\n",
      "   u'retweeted': False,\n",
      "   u'root_query': '#Trump OR #Hilary',\n",
      "   u'symbols': None,\n",
      "   u'text': u'Trump interrupts summation by re-looping back into the full, rambling glory of the body of his speech.',\n",
      "   u'user': {'_id': 167535120,\n",
      "             u'created_at': u'Fri Jul 16 21:00:58 +0000 2010',\n",
      "             u'description': u'I write columns for the National Post and Postmedia/Sun Media newspapers.',\n",
      "             u'favourites_count': 10029,\n",
      "             u'followers_count': 9868,\n",
      "             u'following': False,\n",
      "             u'friends_count': 1252,\n",
      "             u'location': u'',\n",
      "             u'name': u'Michael Den Tandt',\n",
      "             u'screen_name': u'mdentandt',\n",
      "             u'statuses_count': 39569,\n",
      "             u'verified': True},\n",
      "   u'user_mentions': None},\n",
      "  {'_id': 167535120,\n",
      "   u'created_at': u'Fri Jul 16 21:00:58 +0000 2010',\n",
      "   u'description': u'I write columns for the National Post and Postmedia/Sun Media newspapers.',\n",
      "   u'favourites_count': 10029,\n",
      "   u'followers_count': 9868,\n",
      "   u'following': False,\n",
      "   u'friends_count': 1252,\n",
      "   u'location': u'',\n",
      "   u'name': u'Michael Den Tandt',\n",
      "   u'screen_name': u'mdentandt',\n",
      "   u'statuses_count': 39569,\n",
      "   u'verified': True},\n",
      "  None)]\n"
     ]
    }
   ],
   "source": [
    "query = \"#Trump OR #Hilary place:c23\"\n",
    "test_file_path = './queries/#DonaldTrump OR #HillaryClinton OR Trump OR Clinton OR #Trump OR #Clinton/Canada/2016-11-07T19:43:48.144881.json'\n",
    "test = io.open(test_file_path)\n",
    "tweets = json.loads(test.read())\n",
    "\n",
    "min_tweets = [minimize_tweet(tweet, query) for key, tweet in tweets.iteritems()]\n",
    "pprint.pprint(min_tweets[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The insert_many function will open a json file and insert all the objects inside into the database. If the object already exists in the database, check the creation times and update based on the most recent tweet. The tweet is minimized and then added. If the tweet is retweeted, the original tweet is added to the database and also minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def insert_many(filename, query):\n",
    "    '''\n",
    "    Insert multiple items into the Mongo database.\n",
    "    '''\n",
    "    \n",
    "    def insert(document, query, collection):\n",
    "        '''\n",
    "        Insert an object into the Mongo Database, if the item already exists update it based on the created time.\n",
    "        '''\n",
    "        \n",
    "        _id = document['_id']\n",
    "        # find a cursor with the same id\n",
    "        cur = collection.find({'_id': _id}).limit(1)\n",
    "        \n",
    "        #\n",
    "        # if finding the most updated tweets is not required\n",
    "        # can pass upsert=True to collection.update\n",
    "        #\n",
    "        \n",
    "        # make a new entry\n",
    "        if document and cur.count() == 0:\n",
    "            # save query to database in list\n",
    "            collection.insert_one(document)\n",
    "        # check if entry should be updated\n",
    "        else:\n",
    "            new_document = cur.next()\n",
    "            # convert time objects to date time to compare\n",
    "            old_time = convert_datetime(document['created_at'])\n",
    "            new_time = convert_datetime(new_document['created_at'])\n",
    "            \n",
    "            # if the new tweet is more recent, then update\n",
    "            if new_time > old_time:\n",
    "                collection.update({{'_id': _id}}, new_document)\n",
    "\n",
    "        return None\n",
    "\n",
    "    # open the file and load as a json object\n",
    "    f = io.open(filename)\n",
    "    tweets = json.loads(f.read())\n",
    "    \n",
    "    # for each tweet saved in the json file\n",
    "    for tweet_id, raw_tweet in tweets.iteritems():\n",
    "        \n",
    "        # minimize the tweet\n",
    "        tweet, user, retweet_container = minimize_tweet(raw_tweet, query)\n",
    "        \n",
    "        # insert tweet and user\n",
    "        insert(tweet, query, tweet_collection)\n",
    "        insert(user, query, user_collection)\n",
    "        \n",
    "        # if there is a retweet\n",
    "        if retweet_container:\n",
    "            \n",
    "            # minimize the original tweet\n",
    "            tweet, user, _ = minimize_tweet(retweet_container['retweet'], query)\n",
    "            \n",
    "            # insert retweet and user\n",
    "            insert(tweet, query, tweet_collection)\n",
    "            insert(user, query, user_collection)\n",
    "            \n",
    "    f.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the tweets were scraped they were stored in seperate directories in seperate files. The following function will iterate through these directories and open all the files. It will then call the insert many function above and all the tweets into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tweet_db(root, path):\n",
    "    '''\n",
    "    Retrieve all the tweets from the comments, minimize and create database.\n",
    "    '''\n",
    "    os.chdir(path)\n",
    "    root_query = os.getcwd()\n",
    "\n",
    "    for query in os.listdir(os.getcwd()):\n",
    "        # skip hidden files\n",
    "        if query.startswith('.') or query.endswith('.json'):\n",
    "            continue\n",
    "\n",
    "        # move into query directory\n",
    "        os.chdir(query)\n",
    "        # save the path of the current query\n",
    "        query_path = os.getcwd()\n",
    "\n",
    "        # retrieve all the json files not in location directories\n",
    "        for filename in glob.glob('*.json'):\n",
    "            insert_many(filename, query)\n",
    "            # print query, json_file\n",
    "\n",
    "        # iterate through all subdirectories inside of the parent query\n",
    "        for subdir in os.listdir(os.getcwd()):\n",
    "            # skip hidden files and json files (since we already retrieved these above)\n",
    "            if subdir.startswith('.') or subdir.endswith('.json'):\n",
    "                continue\n",
    "            # move into sub directory inside of query (country, timestamp)\n",
    "            os.chdir(subdir)\n",
    "\n",
    "            # retrieve all the json files not in location and timestamp folders\n",
    "            for filename in glob.glob('*.json'):\n",
    "                insert_many(filename, query)\n",
    "                # print query, subdir, json_file\n",
    "\n",
    "            # move out back to query directory\n",
    "            os.chdir(query_path)\n",
    "\n",
    "        # move out of query directory\n",
    "        os.chdir(root_query)\n",
    "\n",
    "    os.chdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets Leading up the election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the current directory\n",
    "path = 'queries'\n",
    "root = os.getcwd()\n",
    "\n",
    "create_tweet_db(root, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets from election day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'election_day'\n",
    "create_tweet_db(root, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the database has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112717\n",
      "87501\n"
     ]
    }
   ],
   "source": [
    "print tweet_collection.count()\n",
    "print user_collection.count()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
